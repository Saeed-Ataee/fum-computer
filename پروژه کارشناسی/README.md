<div align="right" dir="rtl">
## چکیده :
بیگ دیتا یا کلان داده چندسالیست که در ادبیات فناوری اطلاعات به یک اصطلاح فراگیر تبدیل شده است.معمولا ، کلان داده ها به مجموعه داده هایی گفته می شود که توانایی دریافت، اکتساب، مدیریت و پردازش آن ها در یک زمان قابل قبول به وسیله فناوری اطلاعات و ابزارهای نرم افزاری و سخت افزاری سنتی وجود ندارد.عبارت Data Big مدت ها است که برای اشاره به حجم های عظیمی از داده ها که توسط سازمان های بزرگی مانند گوگل یا ناسا ذخیره و تحلیل می شوند مورد استفاده قرار می گیرد. اما به تازگی، این عبارت بیشتر برای اشاره به مجموعه های داده ای بزرگی استفاده می شود که رشد فزاینده ی میزان داده ها به حدی است که با ابزارهای مدیریتی و پایگاه های داده سنتی و معمولی قابل مدیریت نیستند.مشکلات اصلی در کار با این نوع داده ها مربوط به برداشت و جمع آوری، ذخیره سازی، جست وجو، اشتراک گذاری، تحلیل و نمایش آن ها است. این مبحث، به این دلیل هر روز جذابیت و مقبولیت بیشتری پیدا می کند که با استفاده از تحلیل حجم های بیشتری از داده ها، می توان تحلیل های بهتر و پیشرفته تری را برای مقاصد مختلف، از جمله مقاصد تجاری، پزشکی و امنیتی، انجام داد و نتایج مناسب تری را دریافت کرد. تحقیقات در زمینه ی کلان داده ها باید روی چگونگی استخراج ارزش آن ها، چگونگی استفاده از داده ها و چگونگی تبدیل آن ها از گروهی از داده ها به کلان داده ها تمرکز کنند.
هدوپ یک فریم ورک متن باز برای ذخیره سازی امن و پردازش توزیع شده داده های حجیم می باشد، که دارای دو بخش اصلی سیستم فایل توزیع شده هدوپ و موتور پردازش نگاشت و کاهش می باشد. ساختار هدوپ برای کار با داده های بزرگ طراحی شده لذا زمانی که ما دارای فایل های کوچک کلان هستیم هدوپ نمیتواند برخورد مناسبی در مواجه با این مساله از خود نشان دهد و باعث ایجاد بار سنگین بر روی گره اصلی هدوپ و افزایش زمان پردازش نگاشت و کاهش می شود. راهکار های متفاوتی میتواند در برخورد با داده های کوچک به کار برده شود تا باعث بهبود عملکرد ذخیره سازی و پردازش و محاسبات هدوپ شود





## فهرست محتوا

فصل 1 کلان داده
===

 1-1 تعریف	

 1-2 ویژگی ها	

 1-3 انواع داده ها	

 1-4 کاربرد ها	

 1-5 چالش ها	

 1-6 صنایعی که از کلان داده ها استفاده می کنند	

فصل 2 پایگاه داده‌های NoSQL
===

 2-1 مقدمه	

 2-2 مدل‌های داده‌ای در پایگاه داده‌های غیر رابطه‌ای	

 2-3 معیارهای گزینش پایگاه داده NoSQL	

فصل 3 سازگاری داده‌ها در پایگاه‌داده‌های غیررابطه‌ای
===

 3-1 تئوری CAP	

 3-2 ویژگی‌های ACID و BASE	

فصل 4 ماشین مجازی
===

 4-1 معرفی	
 
 4-2 آموزش نصب	

فصل 5 داکر-DOCKER
===

 1-5 معرفی	

 2-5 تفاوت آن با ماشین مجازی	

 3-5 داکر برای چه کسانی مناسب است؟	

 4-5 مکانیزم کاری DOCKER چگونه است؟	33

 5-5 داکر هاب	

 6-5 کانتینر داکر	

 7-5 نصب داکر بر روی اوبونتو	

 8-5 استفاده از داکر برای پروژه	

فصل 6 آپاچی هدوپ  APACHE HADOOP
===

 1-6 معرفی	

 2-6 چرا هدوپ؟	

 3-6 معماری هدوپ	

 4-6 سیستم فایل توزیع شده هدوپ (HDFS)	

 5-6 موتور پردازش نگاشت/کاهش	

 6-6 سرویس مدیریت منابع Yarn	

 7-6 سرویس‌های هدوپ	

 8-6 مراحل پردازش داده‌‌ها توسط هدوپ	

فصل 7 طریقه نصب هدوپ در اوبونتو
===

 1-7 مراحل نصب	

 2-7 اجرای مثال Word Count با هدوپ	

 3-7 استفاده از ماشین مجازی هدوپ	

فصل 8 اکوسیستم هدوپ
===

 1-8 سیستم فایل توزیع شده هدوپ – HDFS	

 2-8 چهارچوب مدیریت منابع YARN	

 3-8 موتور پردازش MapReduce	

 4-8 آپاچی ماهوت APACHE MAHOUT	

 5-8 آپاچی  زو کیپر – APACHE ZOOKEEPER	

فصل 9 آپاچی هایو  Apache Hive
===

 1-9 معرفی	

 2-9 نصب آپاچی Hive بر روی اوبونتو	

فصل 10 آپاچی اسپارک  APACHE SPARK
===

 1-10 معرفی	

 2-10 نصب آپاچی اسپارک بر روی اوبونتو	

 3-10 اجرای مثال WordCount	

فصل 11 آپاچی اچ‌بیس  APACHE HBASE
===

 1-11 معرفی	

 2-11 نصب آپاچی hbase بر روی اوبونتو	

</div>
